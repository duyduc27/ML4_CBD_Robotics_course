{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Về cơ bản của mạng nơ-ron thì chúng ta cần biết về:\n",
    "- ANN (Artificial Neural Networks) và mối liên quan đến sinh học\n",
    "- Ảnh hưởng của thuật toán Perceptron\n",
    "- Thuật toán backpropagation và làm thế nào nó có thể sử dụng để train mạng nơ-ron multi-layer hiệu quả\n",
    "- Làm sao để train mạng nơ-ron sử dụng thư viện Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 Introduction to Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mạng nơ-ron là những khối xây dựng trong hệ thống Deep Learning. Theo đó để thành công trong Deep Learning, ta cần bắt đầu với những kiến thức cơ bản của mạng nơ-ron bao gồm: cấu trúc (architecture), nơ-ron( node types), và thuật toán cho việc học. Sau đó sẽ thảo luận về loại cấu trúc phổ biến nhất: **feedforward neural networks**<br>\n",
    "Mạng ANN đơn giản là lấy cảm hứng từ não người. Mục đích của Deep Learning không phải bắt chước chức năng của não người, mà là lấy một phần trong đó cho phép chúng ta tạo ra những khuôn mẫu tương đồng cho công việc. Não người có xấp xỉ 10 tỷ nơ-ron và mỗi trong số chúng kết nối với khoảng 10 ngàn nơ-ron khác."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 The Perceptron Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Perceptron:\n",
    "    def __init__(self, N, alpha=0.1):\n",
    "        # initialize the weight matrix and store the learning rate\n",
    "        self.W = np.random.randn(N+1)/ np.sqrt(N)\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def step(self, x):\n",
    "        # apply the step function\n",
    "        return 1 if x>0 else 0\n",
    "    \n",
    "    def fit(self, X, y, epochs=10):\n",
    "        # insert a column of 1’s as the last entry in the feature\n",
    "        # matrix -- this little trick allows us to treat the bias\n",
    "        # as a trainable parameter within the weight matrix\n",
    "        X = np.c_[X, np.ones((X.shape[0]))]\n",
    "        \n",
    "        # loop over the desired number of epochs\n",
    "        for epoch in np.arange(0, epochs):\n",
    "            # loop over each individual data point\n",
    "            for (x, target) in zip(X, y):\n",
    "                # take the dot product between the input features\n",
    "                # and the weight matrix, then pass this value\n",
    "                # through the step function to obtain the prediction\n",
    "                p = self.step(np.dot(x, self.W))\n",
    "                \n",
    "                # only perform a weight update if our prediction\n",
    "                # does not match the target\n",
    "                if p!= target:\n",
    "                    # determine the error\n",
    "                    error = p - target\n",
    "                    \n",
    "                    # update the weight matrix\n",
    "                    self.W += -self.alpha*error*x\n",
    "    def predict(self, X, addBias=True):\n",
    "        # ensure our input is a matrix\n",
    "        X = np.atleast_2d(X)\n",
    "        \n",
    "        # check to see if the bias column should be added\n",
    "        if addBias:\n",
    "            # insert a column of 1’s as the last entry in the feature\n",
    "            # matrix (bias)\n",
    "            X = np.c_[X, np.ones((X.shape[0]))]\n",
    "        \n",
    "        # take the dot product between the input features and the\n",
    "        # weight matrix, then pass the value through the step\n",
    "        # function\n",
    "\n",
    "        return  self.step(np.dot(X, self.W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Đầu tiên set ma trận trọng (dòng 5) số W ngẫu nhiên theo phân phối chuẩn, ma trận trọng số có N+1 entries. N là số input feature, 1 là cho bias. Sau đó chia cho căn bậc 2 của N để tỷ lệ lại (scale) ma trận trọng số -> dẫn đến nhanh hội tụ hơn.\n",
    "- Định ra activation function. Ở đây là step function. S(x) > 0 đánh 1 , còn lại đánh 0.\n",
    "- Lặp cho đủ số epoch định ra từ trước. Dòng 25 là hàm tính điểm (scoring function).\n",
    "- Nếu p đúng với target thì thôi. Nếu không đúng thì đi qua Loss function (error). Từ đó cập nhật là ma trận trọng số W.\n",
    "- Dòng 40 check dùng \"bias trick\" chưa, nếu không có gì thay đổi thì add thêm cột toàn số 1 ở entry cuối cùng của matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] data=[0 0], ground-truth=0, pred=0\n",
      "[INFO] data=[0 1], ground-truth=1, pred=1\n",
      "[INFO] data=[1 0], ground-truth=1, pred=1\n",
      "[INFO] data=[1 1], ground-truth=1, pred=1\n"
     ]
    }
   ],
   "source": [
    "# construct the OR dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [1]])\n",
    "\n",
    "p = Perceptron(X.shape[1], alpha=0.1)\n",
    "p.fit(X, y, epochs=20)\n",
    "\n",
    "# now that our network is trained, loop over the data points\n",
    "for (x, target) in zip(X, y):\n",
    "    pred = p.predict(x)\n",
    "    print(\"[INFO] data={}, ground-truth={}, pred={}\".format(x, target[0], pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đúng với dataset bitwise OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] data=[0 0], ground-truth=0, pred=0\n",
      "[INFO] data=[0 1], ground-truth=0, pred=0\n",
      "[INFO] data=[1 0], ground-truth=0, pred=0\n",
      "[INFO] data=[1 1], ground-truth=1, pred=1\n"
     ]
    }
   ],
   "source": [
    "# construct the AND dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [0], [0], [1]])\n",
    "\n",
    "p = Perceptron(X.shape[1], alpha=0.1)\n",
    "p.fit(X, y, epochs=20)\n",
    "\n",
    "# now that our network is trained, loop over the data points\n",
    "for (x, target) in zip(X, y):\n",
    "    pred = p.predict(x)\n",
    "    print(\"[INFO] data={}, ground-truth={}, pred={}\".format(x, target[0], pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đúng với dataset bitwise AND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] data=[0 0], ground-truth=0, pred=1\n",
      "[INFO] data=[0 1], ground-truth=1, pred=0\n",
      "[INFO] data=[1 0], ground-truth=1, pred=0\n",
      "[INFO] data=[1 1], ground-truth=0, pred=0\n"
     ]
    }
   ],
   "source": [
    "# construct the XOR dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "p = Perceptron(X.shape[1], alpha=0.1)\n",
    "p.fit(X, y, epochs=20)\n",
    "\n",
    "# now that our network is trained, loop over the data points\n",
    "for (x, target) in zip(X, y):\n",
    "    pred = p.predict(x)\n",
    "    print(\"[INFO] data={}, ground-truth={}, pred={}\".format(x, target[0], pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Failed với dataset bitwise XOR. Với một single layer Perceptron thì không thể học được, cái ta cần là thêm nhiều layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 Backpropagation and Multi-layer Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.1 The Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/xamg60n.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ví dụ có một feature vector (0,1,1) , các initial weights ở Input layer, mạng 3x3x1. Để propagate values trong mạng và có được output phân loại cuối cùng, ta cần dot product giữa các input (X) và weights. Tiếp theo là dùng một activation fuction để ra output ở stage đó (activation trong trường hợp này là sigmoid function)<br>\n",
    "\n",
    "Tính input của 3 node ở hidden layers<br><br>\n",
    "Với hàm sigmoid là $\\sigma(t) = 1/ (1+ e^{-t})$<br><br>\n",
    "Node thứ nhất:<br> \n",
    "$\\sigma((0 \\times 0.351) + (1 \\times 1.076) + (1 \\times 1.116) )= 0.899$<br>\n",
    "Node thứ hai:<br> \n",
    "$\\sigma((0 \\times -0.097)+ (1 \\times -0.165)+ (1 \\times 0.542)) = 0.593$<br>\n",
    "Node thứ ba:<br> \n",
    "$\\sigma((0 \\times 0.457)+ (1 \\times -0.165)+ (1 \\times -0.331)) = 0.378$<br>\n",
    "\n",
    "**Output của hidden layer** giờ sẽ là **input của layer cuối cùng** trong hình. Để tính toán, ta tiếp tục dot product và theo sau là sigmoid function<br>\n",
    "Node ở layer cuối cùng:<br> \n",
    "$\\sigma((0.899 \\times 0.383)+ (0.593\\times -0.327)+ (0.378 \\times -0.329)) = 0.506$<br>\n",
    "\n",
    "Vậy là output của network là 0.506. Ta định ra kết quả bằng bằng _sigmoid function_ (lớn hơn 0.5 đánh 1, còn lại đánh 0). Vậy là kết quả là 1. Tuy nhiên network không tự tin lắm vì kết quả này gần ngưỡng threshold. Lý tưởng là kết quả gần ở mức 0.98, 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.2 The Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, alpha=0.01):\n",
    "        # initialize the list of weights matrices, then store the\n",
    "        # network architecture and learning rate\n",
    "        self.W = []\n",
    "        self.layers = layers\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # start looping from the index of the first layer but\n",
    "        # stop before we reach the last two layers\n",
    "        for i in np.arange(0, len(layers) -2):\n",
    "            # randomly initialize a weight matrix connecting the\n",
    "            # number of nodes in each respective layer together,\n",
    "            # adding an extra node for the bias\n",
    "            w = np.random.randn(layers[i]+1, layers[i+1] +1)\n",
    "            self.W.append(w/ np.sqrt(layers[i]))\n",
    "            \n",
    "        # the last two layers are a special case where the input\n",
    "        # connections need a bias term but the output does not\n",
    "        w = np.random.randn(layers[-2]+1, layers[-1])\n",
    "        self.W.append(w /np.sqrt(layers[-2]))\n",
    "        \n",
    "    def __repr__(self):\n",
    "        # construct and return a string that represents the network\n",
    "        # architecture\n",
    "        return \"NeuralNetwork: {}\".format(\"-\".join(str(l) for l in self.layers))\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        # compute and return the sigmoid activation value for a\n",
    "        # given input value\n",
    "        return 1/(1+ np.exp(-x))\n",
    "    \n",
    "    def sigmoid_deriv(self,x):\n",
    "        # compute the derivative of the sigmoid function ASSUMING\n",
    "        # that ‘x‘ has already been passed through the ‘sigmoid‘\n",
    "        # function\n",
    "        return x*(1-x)\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, epochs=100, displayUpdate=1000):\n",
    "        # insert a column of 1’s as the last entry in the feature\n",
    "        # matrix -- this little trick allows us to treat the bias\n",
    "        # as a trainable parameter within the weight matrix\n",
    "        X = np.c_[X, np.ones((X.shape[0]))]\n",
    "        \n",
    "        # loop over the desired number of epochs\n",
    "        for epoch in np.arange(0, epochs):\n",
    "            # loop over each individual data point and train\n",
    "            # our network on it\n",
    "            for (x, target) in zip(X, y):\n",
    "                self.fit_partial(x, target)\n",
    "            \n",
    "            # check to see if we should display a training update\n",
    "            if epoch == 0 or (epoch +1) % displayUpdate == 0:\n",
    "                loss = self.calculate_loss(X, y)\n",
    "                print(\"[INFO] epoch={}, loss={}\".format(epoch + 1, loss))\n",
    "                \n",
    "    def fit_partial(self, x, y):\n",
    "        # construct our list of output activations for each layer\n",
    "        # as our data point flows through the network; the first\n",
    "        # activation is a special case -- it’s just the input\n",
    "        # feature vector itself\n",
    "        A = [np.atleast_2d(x)]\n",
    "        \n",
    "        # FEEDFORWARD:\n",
    "        # loop over the layers in the network\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            # feedforward the activation at the current layer by\n",
    "            # taking the dot product between the activation and\n",
    "            # the weight matrix -- this is called the \"net input\"\n",
    "            # to the current layer\n",
    "            net = A[layer].dot(self.W[layer])\n",
    "            \n",
    "            # computing the \"net output\" is simply applying our\n",
    "            # nonlinear activation function to the net input\n",
    "            out = self.sigmoid(net)\n",
    "            \n",
    "            # once we have the net output, add it to our list of\n",
    "            # activations\n",
    "            A.append(out)\n",
    "            \n",
    "        # BACKPROPAGATION\n",
    "        # the first phase of backpropagation is to compute the\n",
    "        # difference between our *prediction* (the final output\n",
    "        # activation in the activations list) and the true target\n",
    "        # value\n",
    "        error= A[-1] -y\n",
    "        # from here, we need to apply the chain rule and build our\n",
    "        # list of deltas ‘D‘; the first entry in the deltas is\n",
    "        # simply the error of the output layer times the derivative\n",
    "        # of our activation function for the output value\n",
    "        D = [error*self.sigmoid_deriv(A[-1])]\n",
    "        \n",
    "        # once you understand the chain rule it becomes super easy\n",
    "        # to implement with a ‘for‘ loop -- simply loop over the\n",
    "        # layers in reverse order (ignoring the last two since we\n",
    "        # already have taken them into account)\n",
    "        for layer in np.arange(len(A)-2, 0, -1):\n",
    "            # the delta for the current layer is equal to the delta\n",
    "            # of the *previous layer* dotted with the weight matrix\n",
    "            # of the current layer, followed by multiplying the delta\n",
    "            # by the derivative of the nonlinear activation function\n",
    "            # for the activations of the current layer\n",
    "            delta = D[-1].dot(self.W[layer].T)\n",
    "            delta = delta* self.sigmoid_deriv(A[layer])\n",
    "            D.append(delta)\n",
    "            \n",
    "        # since we looped over our layers in reverse order we need to\n",
    "        # reverse the deltas\n",
    "        D = D[::-1]\n",
    "        # WEIGHT UPDATE PHASE\n",
    "        # loop over the layers\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            # update our weights by taking the dot product of the layer\n",
    "            # activations with their respective deltas, then multiplying\n",
    "            # this value by some small learning rate and adding to our\n",
    "            # weight matrix -- this is where the actual \"learning\" takes\n",
    "            # place\n",
    "            self.W[layer] += -self.alpha * A[layer].T.dot(D[layer])\n",
    "\n",
    "    def predict(self, X, addBias=True):\n",
    "        # initialize the output prediction as the input features -- this\n",
    "        # value will be (forward) propagated through the network to\n",
    "        # obtain the final prediction\n",
    "        p = np.atleast_2d(X)\n",
    "        # check to see if the bias column should be added\n",
    "        if addBias:\n",
    "            # insert a column of 1’s as the last entry in the feature\n",
    "            # matrix (bias)\n",
    "            p = np.c_[p, np.ones((p.shape[0]))]\n",
    "        # loop over our layers in the network\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            # computing the output prediction is as simple as taking\n",
    "            # the dot product between the current activation value ‘p‘\n",
    "            # and the weight matrix associated with the current layer,\n",
    "            # then passing this value through a nonlinear activation\n",
    "            # function\n",
    "            p = self.sigmoid(np.dot(p, self.W[layer]))\n",
    "        # return the predicted value\n",
    "        return p\n",
    "    \n",
    "    def calculate_loss(self, X, targets):\n",
    "        # make predictions for the input data points then compute\n",
    "        # the loss\n",
    "        targets = np.atleast_2d(targets)\n",
    "        predictions = self.predict(X, addBias=False)\n",
    "        loss = 0.5 * np.sum((predictions - targets) ** 2)\n",
    "        # return the loss\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork: 2-2-1\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork([2, 2, 1])\n",
    "print(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] epoch=1, loss=0.5645996425344315\n",
      "[INFO] epoch=1000, loss=0.009872437593847162\n",
      "[INFO] epoch=2000, loss=0.0029781163745660213\n",
      "[INFO] epoch=3000, loss=0.0016944776613624837\n",
      "[INFO] epoch=4000, loss=0.0011716338967807723\n",
      "[INFO] epoch=5000, loss=0.0008909496790788585\n",
      "[INFO] epoch=6000, loss=0.0007167575962894522\n",
      "[INFO] epoch=7000, loss=0.0005984848314085457\n",
      "[INFO] epoch=8000, loss=0.0005131003400104404\n",
      "[INFO] epoch=9000, loss=0.0004486502936447409\n",
      "[INFO] epoch=10000, loss=0.0003983273657738731\n",
      "[INFO] epoch=11000, loss=0.0003579763393234865\n",
      "[INFO] epoch=12000, loss=0.0003249201033097298\n",
      "[INFO] epoch=13000, loss=0.0002973575966429816\n",
      "[INFO] epoch=14000, loss=0.0002740333945841412\n",
      "[INFO] epoch=15000, loss=0.00025404611125434066\n",
      "[INFO] epoch=16000, loss=0.0002367321087929127\n",
      "[INFO] epoch=17000, loss=0.00022159214443986434\n",
      "[INFO] epoch=18000, loss=0.00020824355333352905\n",
      "[INFO] epoch=19000, loss=0.00019638817691654194\n",
      "[INFO] epoch=20000, loss=0.00018579030985804403\n",
      "[INFO] data=[0 0], ground-truth=0, pred=0.0103, step=0\n",
      "[INFO] data=[0 1], ground-truth=1, pred=0.9894, step=1\n",
      "[INFO] data=[1 0], ground-truth=1, pred=0.9912, step=1\n",
      "[INFO] data=[1 1], ground-truth=0, pred=0.0088, step=0\n"
     ]
    }
   ],
   "source": [
    "# construct the XOR dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# define our 2-2-1 neural network and train it\n",
    "nn = NeuralNetwork([2, 2, 1], alpha=0.5)\n",
    "nn.fit(X, y, epochs=20000)\n",
    "\n",
    "# now that our network is trained, loop over the XOR data points\n",
    "for (x, target) in zip(X, y):\n",
    "    # make a prediction on the data point and display the result\n",
    "    # to our console\n",
    "    pred = nn.predict(x)[0][0]\n",
    "    step = 1 if pred > 0.5 else 0\n",
    "    print(\"[INFO] data={}, ground-truth={}, pred={:.4f}, step={}\".format(x, target[0], pred, step))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mô hình giờ đã tiên đoán đúng với dataset XOR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
