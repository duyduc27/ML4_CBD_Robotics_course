{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Prepare Your Data For Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thường thì bạn sẽ cần tiền xử lý dữ liệu. Đây là một bước yêu cầu. Một cái khó là những thuật toán khác nhau thì có những giả định về dữ liệu khác nhau. Xa hơn nữa, khi bạn đã đi theo các nguyên tắc và chuẩn bị cho dữ liệu của mình. Đôi khi các thuật toán lại đưa ra kết quả tốt hơn khi _không_ tiền xử lý dữ liệu.<br>\n",
    "4 cách tiền xử lý dữ liệu thường thấy:\n",
    "- Rescale data\n",
    "- Standardize data\n",
    "- Normalize data\n",
    "- Binarize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Rescale data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Khi dữ liệu của bạn bị nén lại các attribute với nhiều tỷ lệ khác nhau, rất nhiều thuật toán trong machine learning có lợi ích từ việc rescale tất cả các attribute về cùng một scale (tỷ lệ). Thường thì việc này dẫn đến việc chuẩn hóa về range giữa 0 và 1<br>\n",
    "Điều này thì hữu ích cho việc tối ưu hóa cho các thuật toán được sử dụng chủ yếu trong machine learning như Gradient Descent. Nó cũng hữu ích với các thuật toán có đầu vào là weight input như regression và neural network và các thuật toán sử dụng đo lường khoảng cách như KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.353 0.744 0.59  0.354 0.    0.501 0.234 0.483]\n",
      " [0.059 0.427 0.541 0.293 0.    0.396 0.117 0.167]\n",
      " [0.471 0.92  0.525 0.    0.    0.347 0.254 0.183]\n",
      " [0.059 0.447 0.541 0.232 0.111 0.419 0.038 0.   ]\n",
      " [0.    0.688 0.328 0.354 0.199 0.642 0.944 0.2  ]]\n"
     ]
    }
   ],
   "source": [
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pd.read_csv('pima-indians-diabetes.csv', names=names) # load data\n",
    "array = dataframe.values\n",
    "# separate array into input and output components\n",
    "X = array[:, 0:8] # input\n",
    "Y = array[:,8] # output\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "rescaledX = scaler.fit_transform(X)\n",
    "# summarize transformed data\n",
    "np.set_printoptions(precision=3)\n",
    "print(rescaledX[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau khi rescale tất cả các dữ liệu đều có range từ 0 tới 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Standardize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization là một kỹ thuật hữu ích cho phân phối Gaussian (phân phối chuẩn) và biến những giá trị mean (trung bình) và các std (độ lệch chuẩn) về chuẩn của phân phối Gaussian với mean bằng 0 và std bằng 1.<br>\n",
    "Nó thì phù hợp nhất cho các kĩ thuật giả định là phân phối Gaussian ở các biến đầu vào (input variables) và hoạt động tốt hơn với data đã rescale như linear regression, logistic regression và linear discriminate analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.64   0.848  0.15   0.907 -0.693  0.204  0.468  1.426]\n",
      " [-0.845 -1.123 -0.161  0.531 -0.693 -0.684 -0.365 -0.191]\n",
      " [ 1.234  1.944 -0.264 -1.288 -0.693 -1.103  0.604 -0.106]\n",
      " [-0.845 -0.998 -0.161  0.155  0.123 -0.494 -0.921 -1.042]\n",
      " [-1.142  0.504 -1.505  0.907  0.766  1.41   5.485 -0.02 ]]\n"
     ]
    }
   ],
   "source": [
    "# Standardize data (0 mean, 1 stdev)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(X)\n",
    "rescaledX = scaler.transform(X)\n",
    "# summarize transformed data\n",
    "np.set_printoptions(precision=3)\n",
    "print(rescaledX[:5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Normalize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization Data là việc thay đổi tỷ lệ của từng quan sát (từng row) thành độ dài 1 (còn gọi là chuẩn hóa đơn vị trong linear algebra). Phương pháp tiền xử lý này thì hữu ích với các sparse datasets (bộ dữ liệu nhiều giá trị 0) với những attribute có tỷ lệ khác nhau khi sử dụng thuật toán có weight input values như neural network và đo lường khoảng cách như KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.034 0.828 0.403 0.196 0.    0.188 0.004 0.28 ]\n",
      " [0.008 0.716 0.556 0.244 0.    0.224 0.003 0.261]\n",
      " [0.04  0.924 0.323 0.    0.    0.118 0.003 0.162]\n",
      " [0.007 0.588 0.436 0.152 0.622 0.186 0.001 0.139]\n",
      " [0.    0.596 0.174 0.152 0.731 0.188 0.01  0.144]]\n"
     ]
    }
   ],
   "source": [
    "# Normalize data (length of 1)\n",
    "from sklearn.preprocessing import Normalizer\n",
    "scaler = Normalizer().fit(X)\n",
    "rescaledX = scaler.transform(X)\n",
    "# summarize transformed data\n",
    "np.set_printoptions(precision=3)\n",
    "print(rescaledX[:5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Binarize Data (Make Binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Biến đổi dữ liệu sử dụng binary threshold. Tất cả giá trị ở trên threshold được đánh là 1, nhỏ hơn hoặc bằng đánh là 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1. 0. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 0. 1. 1. 1.]\n",
      " [1. 1. 1. 0. 0. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 1. 1. 1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# binarization\n",
    "from sklearn.preprocessing import Binarizer\n",
    "scaler = Binarizer(threshold=0.0).fit(X)\n",
    "rescaledX = scaler.transform(X)\n",
    "# summarize transformed data\n",
    "np.set_printoptions(precision=3)\n",
    "print(rescaledX[:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ở đây sử dụng threshold bằng 0, nên trên 0 đánh là 1 , bằng hoặc nhỏ hơn 0 đánh là 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Feature Selection For Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Univariate Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Những statistical test có thể được sử dụng để chọn ra những feature có mối quan hệ mạnh mẽ nhất với output.<br>\n",
    "Bên dưới sử dụng kiểm thử thống kê chi-squared ($chi^2$ ) cho những feature **không** có giá trị **âm**. Chọn ra 4 feature tốt nhất."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Điểm các feature:\n",
      " [ 111.52  1411.887   17.605   53.108 2175.565  127.669    5.393  181.304]\n",
      "\n",
      "[[148.    0.   33.6  50. ]\n",
      " [ 85.    0.   26.6  31. ]\n",
      " [183.    0.   23.3  32. ]\n",
      " [ 89.   94.   28.1  21. ]\n",
      " [137.  168.   43.1  33. ]]\n"
     ]
    }
   ],
   "source": [
    "# Feature Extraction with Univariate Statistical Tests (Chi-squared for classification)\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "# feature extraction\n",
    "test = SelectKBest(score_func=chi2, k=4)\n",
    "fit = test.fit(X,Y)\n",
    "# summarize score\n",
    "np.set_printoptions(precision=3)\n",
    "print('Điểm các feature:\\n', fit.scores_)\n",
    "print()\n",
    "features = fit.transform(X)\n",
    "# summarize selected features\n",
    "print(features[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 feature có quan hệ mạnh mẽ nhất với ouput là 4 feature có điểm score cao nhất tương ứng: plas, test, mass và age (ta có thể biết bằng cách nối theo dataframe bằng tay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recursive Feature Elimination (or RFE) hoạt động theo kiểu đệ quy xóa các attribute và xây dựng mô hình với các attribute còn lại. Nó sử dụng độ chính xác của mô hình để xác định attribute nào đóng góp nhiều nhất đến sự tiên đoán của target attribute.<br>\n",
    "Bên dưới là ví dụ của RFE trong mô hình Logistic Regression để chọn ra top 3 feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features: 3\n",
      "Selected Features: [ True False False False False  True  True False]\n",
      "Feature Ranking: [1 2 3 5 6 1 1 4]\n",
      "\n",
      "[[ 6.    33.6    0.627]\n",
      " [ 1.    26.6    0.351]\n",
      " [ 8.    23.3    0.672]\n",
      " [ 1.    28.1    0.167]\n",
      " [ 0.    43.1    2.288]]\n"
     ]
    }
   ],
   "source": [
    "# Feature Extraction with RFE\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "rfe = RFE(model, 3)\n",
    "fit = rfe.fit(X, Y)\n",
    "print(\"Num Features: %d\" % fit.n_features_)\n",
    "print(\"Selected Features: %s\" % fit.support_)\n",
    "print(\"Feature Ranking: %s\" % fit.ranking_)\n",
    "print()\n",
    "features = fit.transform(X)\n",
    "print(features[:5, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class'], dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Khi chọn 3 feature, RFE đã chọn preg, mass, pedi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis (or PCA) sử dụng đại số tuyến tính để biến đổi dữ liệu về một dạng nén. Nhìn chung đây được gọi là kĩ thuật giảm dữ liệu. Đặc tính của PCA là chọn số chiều hoặc thành phần quan trọng trong kết quả được biến đổi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance: [0.889 0.062 0.026]\n",
      "\n",
      "[[-2.022e-03  9.781e-02  1.609e-02  6.076e-02  9.931e-01  1.401e-02\n",
      "   5.372e-04 -3.565e-03]\n",
      " [-2.265e-02 -9.722e-01 -1.419e-01  5.786e-02  9.463e-02 -4.697e-02\n",
      "  -8.168e-04 -1.402e-01]\n",
      " [-2.246e-02  1.434e-01 -9.225e-01 -3.070e-01  2.098e-02 -1.324e-01\n",
      "  -6.400e-04 -1.255e-01]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3)\n",
    "fit = pca.fit(X)\n",
    "# summarize components\n",
    "print(\"Explained Variance: %s\" % fit.explained_variance_ratio_)\n",
    "print()\n",
    "print(fit.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bạn có thể thấy bộ dữ liệu bị biến đổi (n_components =3) bộc lộ một chút nét tương đồng với dữ liệu gốc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4  Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.114 0.219 0.096 0.087 0.07  0.153 0.123 0.138]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Feature Importance with Extra Trees Classifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X,Y)\n",
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng ta được cho những điểm quan trọng của các attribute, mà điểm cao thì quan trọng hơn. Điểm số đề xuất ra những attribute quan trọng là plas, mass, age."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
