{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOC\n",
    "[I. Improve Performance with Ensembles](#I.-Improve-Performance-with-Ensembles)<br><br>\n",
    "[1.1 Bagging Algorithms](#1.1-Bagging-Algorithms)<br>\n",
    "[1.1.1 Bagged Decision Trees](#1.1.1-Bagged-Decision-Trees)<br>\n",
    "[1.1.2 Random Forest](#1.1.2-Random-Forest)<br>\n",
    "[1.1.3 Extra Trees](#1.1.3-Extra-Trees)<br><br>\n",
    "[1.2 Boosting Algorithms](#1.2-Boosting-Algorithms)<br>\n",
    "[1.2.1 AdaBoost](#1.2.1-AdaBoost)<br>\n",
    "[1.2.2 Stochastic Gradient Boosting](#1.2.2-Stochastic-Gradient-Boosting)<br><br>\n",
    "[1.3 Voting Ensemble](#1.3-Voting-Ensemble)<br><br>\n",
    "[II. Improve Performance with Algorithm Tuning](#II.-Improve-Performance-with-Algorithm-Tuning)<br>\n",
    "[2.1 Machine Learning Algorithm Parameters](#2.1-Machine-Learning-Algorithm-Parameters)<br>\n",
    "[2.2 Grid Search Parameter Tuning](#2.2-Grid-Search-Parameter-Tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Improve Performance with Ensembles\n",
    "Sử dụng ensembles để boost accuracy trên tập dữ liệu của bạn.<br>\n",
    "Kết hợp nhiều mô hình vào các tiên đoán ensemble. 3 phương thức phổ biến nhất cho việc kết hợp các tiên đoán từ các mô hình khác nhau là:\n",
    "- Bagging: xây dựng nhiều mô hình (lý tưởng là cùng một loại) từ những subsample khác nhau của tập train.\n",
    "- Boosting: xây dựng nhiều mô hình (lý tưởng là cùng một loại), mỗi cái trong số đó học để fix sai số trong tiên đoán của mô hình trước trong chuỗi các mô hình.\n",
    "- Voting: xây dựng nhiều mô hình (lý tưởng là khác loại) và những thống kê đơn giản (ví dụ như tính mean) được sử dụng để kết hợp các tiên đoán."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Bagging Algorithms\n",
    "Bagging liên quan đến việc lấy nhiều mẫu từ bộ dữ liệu train (với sự thay thế) và train một mô hình trên từng mẫu đó. Kết quả cuối cùng là trung bình các tiên đoán của tất cả các mô hình con.\n",
    "- Bagged Decision Trees.\n",
    "- Random Forest.\n",
    "- Extra Trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Bagged Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging thể hiện tốt nhất với những thuật toán có variance cao. Một ví dụ phổ biến nhất là Decision Tree, thường được cấu trúc mà chưa làm gọn. Ở ví dụ bên dưới **100** cây đã được tạo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preg</th>\n",
       "      <th>plas</th>\n",
       "      <th>pres</th>\n",
       "      <th>skin</th>\n",
       "      <th>test</th>\n",
       "      <th>mass</th>\n",
       "      <th>pedi</th>\n",
       "      <th>age</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   preg  plas  pres  skin  test  mass   pedi  age  class\n",
       "0     6   148    72    35     0  33.6  0.627   50      1\n",
       "1     1    85    66    29     0  26.6  0.351   31      0\n",
       "2     8   183    64     0     0  23.3  0.672   32      1\n",
       "3     1    89    66    23    94  28.1  0.167   21      0\n",
       "4     0   137    40    35   168  43.1  2.288   33      1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pd.read_csv('pima-indians-diabetes.csv', names=names)\n",
    "array = dataframe.values\n",
    "X = array[:, :8] # input\n",
    "Y = array[:, 8] # output\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.770745044429255\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "scoring = 'accuracy'\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "cart = DecisionTreeClassifier()\n",
    "num_trees = 100\n",
    "model = BaggingClassifier(base_estimator=cart, n_estimators=num_trees, random_state=seed)\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau khi chạy ví dụ trên, ta thấy kết quả mô hình theo độ chính xác đo được khá tốt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest là phần mở rộng của bagged decision trees. Các mẫu của bộ dữ liệu train vẫn được lấy bằng cách thay thế Nhưng các cây được cấu trúc theo cách giảm mối tương quan giữa các cá thể phân loại. Cụ thể, thay vì chọn một cách tham lam split point tốt nhất trong tất cả các feature thuộc cấu trúc từng cây, thì chỉ có **một bộ** các feature **ngẫu nhiên** được dùng cho việc tách.<br>\n",
    "Bên dưới là ví dụ cho **100 cây** với bộ subset các feature ngẫu nhiên là **3**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7733766233766234\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "scoring = 'accuracy'\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "num_trees = 100\n",
    "max_features = 3\n",
    "model = RandomForestClassifier(n_estimators=num_trees, max_features=max_features, random_state=seed)\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Extra Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra Trees là một sửa đổi từ bagging. Ở đó những cây ngẫu nhiên thì được cấu trúc từ các mẫu của bộ dữ liệu train.<br>\n",
    "Ví dụ bên dưới cho **100 cây** và các điểm tách từ **7** features ngẫu nhiên."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.762987012987013\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "scoring = 'accuracy'\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "num_trees = 100\n",
    "max_features = 7\n",
    "model = ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features, random_state=seed)\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Boosting Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Các thuật toán Boosting ensemble tạo ra một chuỗi các mô hình, trong đó chúng sẽ cố sửa chữa những cái sai của mô hình trước đó theo chuỗi. Một khi được tạo ra, các mo hình tạo ra các tiên đoán được trọng số bởi độ chính xác của chúng và các kết quả này được kết hợp lại để tạo ra tiên đoán cuối cùng cho output.<br>\n",
    "Hai thuật toán boosting ensemble phổ biến nhất là :\n",
    "- AdaBoost.\n",
    "- Stochastic Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost có lẽ là thuật toán boosting ensemble thành công. Nó nhìn chung hoạt động bằng cách trọng số các thực thể trong bộ dữ liệu bằng việc chúng khó hoặc dễ phân loại như thế nào. Cho phép thuật toán này hoặc quan tâm hơn hoặc ít quan tâm hơn trong cấu trúc của các mô hình tiếp theo.<br>\n",
    "Ví dụ bên dưới cấu trúc **30 cây decision trees**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.760457963089542\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "scoring = 'accuracy'\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "num_trees = 30\n",
    "cart = DecisionTreeClassifier(max_depth=1)\n",
    "model = AdaBoostClassifier(base_estimator=cart, n_estimators=num_trees, random_state=seed)\n",
    "# nếu không chọn base estimator thì mặc định là DecisionTreeClassifier(max_depth=1)\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Stochastic Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Gradient Boosting là một trong những kĩ thuật ensemble phức tạp nhất. Nó cũng là một kĩ thuật được chứng minh là một trong những kĩ thuật tốt nhất cho việc cải thiện khả năng biểu diễn của mô hình qua ensemble.\n",
    "Ví dụ bên dưới sử dụng Stochastic Gradient Boosting với **100 cây decision trees**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7733937115516063\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "scoring = 'accuracy'\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "num_trees = 100\n",
    "cart = DecisionTreeClassifier(max_depth=1)\n",
    "model = GradientBoostingClassifier(n_estimators=num_trees, random_state=seed, init=cart)\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Voting Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voting là một trong những cách đơn giản nhất kết hợp các tiên đoán từ nhiều thuật toán machine learning. Nó hoạt động bằng việc tạo ra 2 hay nhiều mô hình chuẩn từ bộ dữ liệu train. Một Voting Classifier có thể dùng để gói các mô hình và trung bình các tiên đoán từ các mô hình con và từ đó tạo ra các tiên đoán cho dữ liệu chưa được nhìn thấy.<br>\n",
    "Các tiên đoán của các mô hình có thể trọng số nhưng cụ thể trọng số từng mô hình đó thì khó. Một trong những phương thức nâng cao hơn là học cách làm sao để trọng số tốt nhất cho các tiên đoán từ các mô hình con. Cái này được gọi là stacking và trong tài liệu này thì scikit-learn vẫn chưa hỗ trợ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7656015037593986\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "scoring = 'accuracy'\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "# create the sub models\n",
    "estimators = []\n",
    "estimators.append(('LR', LogisticRegression(solver='liblinear'))) # model 1\n",
    "estimators.append(('CART', DecisionTreeClassifier(max_depth=1))) # model 2\n",
    "estimators.append(('SVM', SVC(gamma='auto'))) # model 3\n",
    "# create the ensemble model\n",
    "ensemble = VotingClassifier(estimators=estimators)\n",
    "results = cross_val_score(ensemble, X, Y, cv=kfold, scoring=scoring)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Improve Performance with Algorithm Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Các mô hình machine learning thì có những tham số cụ thể mà hành vi của chúng có thể được điều chỉnh cho một bài toán cụ thể. Các mô hình có thể có nhiều tham số và tìm ra được bộ tham số kết hợp tốt nhất thì giải quyết được bài toán tốt hơn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Machine Learning Algorithm Parameters\n",
    "\n",
    "Điều chỉnh thuật toán là bước cuối cùng trong quá trình ứng dụng machine learning trước khi hoàn tất mô hình của bạn. Nó thỉnh thoảng được gọi là tối ưu hóa hyperparameter, ở đây thông số của thuật toán được hiểu là hyperparameter. Còn các hệ số được tìm ra trong thuật toán machine learning là thông số<br>\n",
    "Tối ưu hóa là phương pháp thử-sai cho bài toán đặt ra. Có 2 phương thức đơn giản cho việc điều chỉnh thông số thuật toán là:\n",
    "- Grid Search Parameter Tuning.\n",
    "- Random Search Parameter Tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Grid Search Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search là một cách tiếp cận điều chỉnh tham số mà ở đó sẽ xây dựng và đánh giá **1** mô hình cho từng tham số kết hợp trong grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RidgeClassifier(alpha=0.1, class_weight={0: 0.5, 1: 0.5}, copy_X=True,\n",
      "                fit_intercept=True, max_iter=None, normalize=False,\n",
      "                random_state=None, solver='auto', tol=0.001)\n",
      "{'alpha': 0.1, 'class_weight': {1: 0.5, 0: 0.5}}\n",
      "0.7708646616541354\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "alphas = [1,0.1,0.01,0.001,0.0001,0] # test with alpha set\n",
    "class_weights = [{1: 0.7, 0:0.3}, {1:0.5, 0:0.5}, {1:0.4, 0:0.6}] # test with class weight set\n",
    "param_grid = dict(alpha=alphas, class_weight = class_weights)\n",
    "model = RidgeClassifier()\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
    "grid.fit(X,Y)\n",
    "print(grid.best_estimator_)\n",
    "print(grid.best_params_ )\n",
    "# test with new parameter\n",
    "results = cross_val_score(grid.best_estimator_, X, Y, cv=kfold, scoring=scoring)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vậy sau khi grid search cv trên bộ tham số alpha và class_weight thì ta tìm được 'alpha': 0.1, 'class_weight': {1: 0.5, 0: 0.5} là những tham số tốt nhất _trong bộ tham số_ ở mô hình này."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
